#+title: Kernel Collocation Excercise
#+author: Jonathan Ulmer (3545737)
#+bibliography: ~/org/roam/papers/bibliography.bib
#+latex_compiler: xelatex
#+latex_header: \newcommand{\RR}{\mathbb{R}}
#+latex_header: \usepackage{amsmath}
#+latex_header: \usepackage{amssymb}
#+latex_header: \newtheorem{remark}{Remark}
#+latex_header:\usepackage[T1]{fontenc}
#+latex_header: \usepackage{unicode-math}
#+latex_header: \setmonofont{DejaVu Sans Mono}[Scale=0.8]
#+Property: header-args:julia :eval never-export :async t :session *julia* :exports both :tangle src/snippets.jl :comments org

* Preamble
#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.3f"
  (mapcar (lambda (row)
            (mapcar (lambda (cell)
                      (if (numberp cell)
                          (format fmt cell)
                        cell))
                    row))
          tbl)
#+end_src

#+RESULTS: round-tbl

#+RESULTS:

#+begin_src julia :tangle src/kernel.jl
module PDESystem

end
#+end_src

#+RESULTS:

#+begin_src julia :results output
test()
#+end_src

#+RESULTS:
: hello world

#+begin_src julia :results file graphics :file "images/plot.png"

using Plots
f(x) = exp(x) + 10 * sin(x)
plot(f)
savefig("images/plot.png")
#+end_src

#+RESULTS:
[[file:images/plot.png]]
#+begin_src julia
using KernelAbstractions

@kernel function Δ_kernel(A , Δ)
    I = @index(Global , Cartesian)
    Ix = oneunit(I)
    Δ[I] = 0
    dimensions = ndims(A)
    for i in 1:dimensions
        zero(I)
        In = CartesianIndex()
        Δ[I] += A[I + In] - A[I]
    end
end
#+end_src

#+RESULTS:
: Δ_kernel (generic function with 4 methods)


#+begin_src julia
dt = 0.1
for t in 0:0.1:10
x += dx * dt
end
#+end_src


* Regression Approach
given \( \hat{X}:=\left\{ x_j \right\}_{j=1}^n \subset\RR ^d\) we aim

\begin{align}
\label{eq:approx}
u_h(x) &= \sum_{j=1}^{n} a_j k(x_j,x)
\end{align}

correspondingly we are able to directly compute

\begin{align*}
\nabla_x u(x) &= \sum_{j=1}^n a_j \nabla_x  k(x_j ,x) \\
- \nabla_x \cdot \left( a(x) \nabla_x u(x) \right) &= - \left< \nabla_x a(x) , \nabla_x u(x) \right> - a(x) \Delta_x u(x) \\
&=  - \sum_{j=1}^{n} a_j \left( \left< \nabla_x a(x) , \nabla_x k(x_j,x)  \right> - a(x) \Delta_x k(x_j,x)\right)
\end{align*}
where \(\nabla_x , \Delta_x\) are the partial gradients and laplacians with respect to the second argument of \(k(x_j, \cdot )\).
for a radial basis function \(\phi (r) \in  C^2(\RR)\)  and a corresponding RBF kernel \(k(x,x') := \phi (\|x-x'\|)\) they can be computed trivially
\begin{align}
\label{eq:2}
\nabla_x k(x',x) &= \phi'(\|x-x'\|)x\\
\Delta_x k(x',x) &= \phi''(\|x-x'\|) + \phi'(\|x-x'\|) \left< x,x \right>\\
\end{align}
** Kernel Implementation Implementation
#+begin_src julia
function rbf_gaussian(r, ::Val{γ}) where γ
    exp(- γ * r)
    end
function rbf_gaussian′(r , ::Val{γ}) where γ
    - γ * exp(- γ * r)
    end
function rbf_gaussian′′(r , ::Val{γ}) where γ
    γ^2 * exp(- γ * r)
    end
function k_gauss(x , x̂)
    rbf_gaussian(norm(x-x̂) , Val(0.1))
    end
function ∇k_gauss(x,x̂)
    rbf_gaussian′(norm(x-x̂) , Val(0.1)) * x
    end
function Δk_gauss(x,x̂)
    rbf_gaussian′′(norm(x-x̂) , Val(0.1))  + rbf_gaussian′(norm(x-x̂) , Val(0.1)) *  dot(x , x)
    end
#+end_src

#+RESULTS:
: Δk_gauss (generic function with 1 method)

#+begin_src julia :results file graphics :file "images/gauss-rbf.png"
using GLMakie
X = range(-5 , 5 , 100)
Y = range(-5 , 5 , 100)
using LinearAlgebra

fig = Figure()
ax = Axis3(fig[1,1] , aspect=:equal)

gauss(x) = k_gauss(x , [0,0])
z = [gauss([x,y]) for x in X , y in Y]
surface!(ax, x,y, z)
save("images/gauss-rbf.png",fig )
#+end_src

#+RESULTS:
[[file:images/gauss-rbf.png]]


* PDE System
such that they satisfy the following system


\begin{align}
\label{eq:pde}
- \nabla  \left( a(x) \nabla u(x) \right) &= f(x) & \text{in} \quad \Omega \\
u(x) &= g_D(x) & \text{on} \quad  \Gamma_D \\
\left( a(x) \nabla u(x)  \right) \cdot  \vec{n}(x) &= g_N & \text{on} \quad \Gamma_N
\end{align}
where
#+begin_src julia
a(x::SVector{3}) = x[1] + 2
∇a(x::SVector{3}) = SVector{3}(1.,0.,0.)
#+end_src

#+RESULTS:
: ∇a (generic function with 2 methods)

** Optimization Problem
Aim of the regression is then, to minimize
\begin{align}
\label{eq:3}
\mathcal{J}(\vec{a} ; \hat{X})
\end{align}
If the solution to the PDE system  exists, then finding the solution is equivalent to minimizing the following functional:
\begin{align}
\label{eq:4}
\mathcal{J}(u) &= \int_{\Omega}
\end{align}
leading to the linear system
\begin{align}
\label{eq:linear-system}
\left< u_h , k(x_j , \cdot) \right>_{\mathcal{H}_k} &= \left< f , k(x_j , \cdot) \right>_{\mathcal{H}_k}
\end{align}
\begin{align}
\label{eq:5}
- \sum_{j=1}^{n} a_j \left( \left< \nabla_x a(x_i) , \nabla_x k(x_j,x_i)  \right> - a(x_i) \Delta_x k(x_j,x_i)\right) &= f(x_i)
\end{align}
* Linear Sytem
#+begin_src julia :tangle kernel.jl
using StaticArrays
using KernelAbstractions
@kernel function assemble_matrix!(A ,@Const(X) , k, ∇k , Δk , a , ∇a)
    # boilerplate
    Iᵢⱼ = @index(Global , Cartesian)
    @inbounds xᵢ= SVector{3}(view(X , : , Iᵢⱼ[1]))
    @inbounds xⱼ= SVector{3}(view(X , : , Iᵢⱼ[2]))
    # element computation
    A[Iᵢⱼ] = ∇a(xᵢ)⋅∇k(xᵢ,xⱼ) -  a(xᵢ)Δk(xⱼ,xᵢ)
    end
#+end_src

#+RESULTS:

#+begin_src julia :results table
X = rand(3,10)
A = rand(10,10)
assemble = assemble_matrix!( CPU() , 64 , size(A))
assemble(A  , X , k_gauss , ∇k_gauss , Δk_gauss , a , ∇a)
A
#+end_src

#+RESULTS:
| 0.216 | 0.302 | 0.329 | 0.036 | 0.159 | 0.043 | 0.165 | -0.033 | 0.071 | 0.193 |
| 0.201 | 0.320 | 0.344 | 0.042 | 0.156 | 0.048 | 0.156 | -0.025 | 0.078 | 0.194 |
| 0.199 | 0.318 | 0.357 | 0.037 | 0.156 | 0.042 | 0.153 | -0.034 | 0.074 | 0.196 |
| 0.196 | 0.289 | 0.317 | 0.058 | 0.162 | 0.060 | 0.156 | -0.009 | 0.091 | 0.197 |
| 0.202 | 0.305 | 0.340 | 0.023 | 0.168 | 0.029 | 0.153 | -0.055 | 0.063 | 0.203 |
| 0.204 | 0.297 | 0.324 | 0.051 | 0.163 | 0.058 | 0.161 | -0.017 | 0.084 | 0.198 |
| 0.214 | 0.300 | 0.327 | 0.037 | 0.160 | 0.044 | 0.167 | -0.032 | 0.072 | 0.194 |
| 0.198 | 0.283 | 0.309 | 0.059 | 0.164 | 0.064 | 0.159 | -0.005 | 0.091 | 0.196 |
| 0.188 | 0.272 | 0.296 | 0.072 | 0.159 | 0.074 | 0.154 |  0.014 | 0.107 | 0.190 |
| 0.200 | 0.310 | 0.346 | 0.026 | 0.163 | 0.032 | 0.152 | -0.051 | 0.066 | 0.210 |
#+TBLFM: @1$1..@>$>=@0$0;%.3f


