#+title: Kernel Collocation Excercise
#+author: Jonathan Ulmer (3545737)
#+bibliography: ~/org/roam/papers/bibliography.bib
#+latex_compiler: xelatex
#+latex_header: \newcommand{\RR}{\mathbb{R}}
#+latex_header: \usepackage{amsmath}
#+latex_header: \usepackage{amssymb}
#+latex_header: \newtheorem{remark}{Remark}
#+latex_header:\usepackage[T1]{fontenc}
#+latex_header: \usepackage{unicode-math}
#+latex_header: \setmonofont{DejaVu Sans Mono}[Scale=0.8]
#+Property: header-args:julia :eval never-export :async t :session *julia* :exports both :tangle src/snippets.jl :comments org

#+begin_export html
<div style="display:none">
\(
\newcommand{\RR}{\mathbb{R}}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{remark}{Remark}
\)
</div>
#+end_export

* Distance Matrix Comutation
#+begin_src julia
using KernelAbstractions
using StaticArrays
using Distributed
@kernel function distance_matrix!(A ,@Const(X_1) , @Const(X_2))
    # boilerplate
    Iᵢⱼ = @index(Global , Cartesian)
    @inbounds xᵢ= SVector{3}(view(X_1 , : , Iᵢⱼ[1]))
    @inbounds xⱼ= SVector{3}(view(X_2 , : , Iᵢⱼ[2]))
    # element computation
    @inbounds d = xᵢ - xⱼ
    @inbounds A[Iᵢⱼ] = d' * d
    end



function distM(X₁ ,X₂)
    A = KernelAbstractions.zeros(get_backend(X_1) , Float32 , size(X₁,2) , size(X₂,2))
    dist_kernel! = distance_matrix!(get_backend(A) , 256 , size(A))
    dist_kernel!(A ,X₁ , X₂ )
    KernelAbstractions.synchronize(get_backend(A))
    return A
end

function distK(X_1 , X_2)
norm_1 = sum(X_1.^2 ; dims=1)
norm_2 = sum(X_2.^2 ; dims=1)
distM = -2*(X_1'*X_2) .+ norm_1' .+ norm_2
end
#+end_src

#+RESULTS:
: distK (generic function with 1 method)

#+begin_src julia :exports code :results none
using CUDA
X_1 = rand(3,10_000) |> cu
X_2 = rand(3,10_000) |> cu

#+end_src


#+begin_src julia
using BenchmarkTools
@benchmark distK(X_1 , X_2)
#+end_src

#+RESULTS:
#+begin_example
BenchmarkTools.Trial: 1012 samples with 1 evaluation per sample.
 Range (min … max):  49.360 μs … 87.566 ms  ┊ GC (min … max): 0.00% …  0.55%
 Time  (median):     63.765 μs              ┊ GC (median):    0.00%
 Time  (mean ± σ):    4.965 ms ± 16.091 ms  ┊ GC (mean ± σ):  1.42% ± 22.26%

  █                                                        ▁▁
  █▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▁██ ▇
  49.4 μs      Histogram: log(frequency) by time      56.6 ms <

 Memory estimate: 15.81 KiB, allocs estimate: 557.
#+end_example

#+begin_src julia
@benchmark distM(X_1, X_2)
#+end_src

#+RESULTS:
#+begin_example
BenchmarkTools.Trial: 2524 samples with 1 evaluation per sample.
 Range (min … max):  1.922 ms …  3.315 ms  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     1.945 ms              ┊ GC (median):    0.00%
 Time  (mean ± σ):   1.978 ms ± 81.906 μs  ┊ GC (mean ± σ):  2.91% ± 9.02%

  ▂▆███▇▅▃▂▁       ▁▂▁▁▁         ▁ ▁  ▁▂▂▁▁                  ▁
  ████████████▇██████████▇█▅▇▇▇█████▇███████▆▅▆▁▅▇██▅▅▆▆▅▆▅▅ █
  1.92 ms      Histogram: log(frequency) by time     2.22 ms <

 Memory estimate: 2.25 KiB, allocs estimate: 81.
#+end_example

* Regression Approach
given \( \hat{X}:=\left\{ x_j \right\}_{j=1}^n \subset\RR ^d\) we aim

\begin{align}
\label{eq:approx}
u_h(x) &= \sum_{j=1}^{n} a_j k(x_j,x)
\end{align}

correspondingly we are able to directly compute

\begin{align*}
\nabla_x u(x) &= \sum_{j=1}^n a_j \nabla_x  k(x_j ,x) \\
- \nabla_x \cdot \left( a(x) \nabla_x u(x) \right) &= - \left< \nabla_x a(x) , \nabla_x u(x) \right> - a(x) \Delta_x u(x) \\
&=  - \sum_{j=1}^{n} a_j \left( \left< \nabla_x a(x) , \nabla_x k(x_j,x)  \right> - a(x) \Delta_x k(x_j,x)\right)
\end{align*}
where \(\nabla_x , \Delta_x\) are the partial gradients and laplacians with respect to the second argument of \(k(x_j, \cdot )\).
for a radial basis function \(\phi (r^2) \in  C^2(\RR)\)  and a corresponding RBF kernel \(k(x,x') := \phi (\|x-x'\|^2)\) they can be computed trivially
\begin{align}
\label{eq:2}
\nabla_x k(x',x) &= 2*(x - x')*\phi'(\|x-x'\|^2)\\
\Delta_x k(x',x) &= 2*d*\phi'(\|x-x'\|^2) + 4*\phi''(\|x-x'\|^2) \left\|x-x'\right\|^2\\
\end{align}
where \(d\) is the dimenstion of \(x\)
* Kernel Implementation Implementation
#+begin_src julia
using StaticArrays
function rbf_gaussian(r, ::Val{γ}) where γ
    exp(- γ * r)
    end
function rbf_gaussian′(r , ::Val{γ}) where γ
    - γ * exp(- γ * r)
    end
function rbf_gaussian′′(r , ::Val{γ}) where γ
    γ^2 * exp(- γ * r)
    end
function k_gauss(x ::SVector{N}, x̂ ::SVector{N}) where N
    rbf_gaussian(norm(x-x̂)^2 , Val(1.))
    end
function ∇k_gauss(x::SVector{N},x̂::SVector{N}) where N
    rbf_gaussian′(norm(x-x̂)^2 , Val(1.)) * 2*(x-x̂)
    end
function Δk_gauss(x::SVector{N},x̂::SVector{N}) where N
    d = dot(x-x̂,x-x̂)
    4*rbf_gaussian′′(d , Val(1.))  + 4*rbf_gaussian′(d , Val(1.)) * d
    end
#+end_src

#+RESULTS:
: Δk_gauss (generic function with 1 method)

#+begin_src julia :results file graphics :file "images/gauss-rbf.png"
using GLMakie
X = range(-5 , 5 , 100)
Y = range(-5 , 5 , 100)
using LinearAlgebra

fig = Figure()
ax = Axis3(fig[1,1] , aspect=:equal)

gauss(x) = k_gauss(x , [0,0])
z = [gauss([x,y]) for x in X , y in Y]
surface!(ax, X,Y, z)
save("images/gauss-rbf.png",fig )
#+end_src

#+RESULTS:
[[file:images/gauss-rbf.png]]


* PDE System
such that they satisfy the following system


\begin{align}
\label{eq:pde}
- \nabla  \left( a(x) \nabla u(x) \right) &= f(x) & \text{in} \quad \Omega \\
u(x) &= g_D(x) & \text{on} \quad  \Gamma_D \\
\left( a(x) \nabla u(x)  \right) \cdot  \vec{n}(x) &= g_N & \text{on} \quad \Gamma_N
\end{align}
where
#+begin_src julia
using StaticArrays
a(x::SVector{2}) = x[1] + 2
∇a(x::SVector{2}) = SVector{2}(1.,0.)
#+end_src

#+RESULTS:
: ∇a (generic function with 1 method)

#+begin_src julia
α = 2.
β = 0.5
f(x::SVector{2} , ::Val{α}) where α = - α*norm(x ,2)^(α - 2)*(3x[1] +4) - α*(α -2) * (x[1] + 2) * norm(x,2)^(α - 3)
g_D(x::SVector{2} , ::Val{α}) where α = norm(x,2)^α
g_N(x::SVector{2} , n::SVector{2} , ::Val{α}) where α = α* norm(x,2.)^(α-2.)*(x[1] +2.) * x ⋅ n
f(x) = f(x,Val(α))
g_D(x) = g_D(x,Val(α))
g_N(x) = g_N(x,Val(α))
#+end_src

#+RESULTS:
: g_N (generic function with 2 methods)

** Optimization Problem
Aim of the regression is then, to minimize
\begin{align}
\label{eq:3}
\mathcal{J}(\vec{a} ; \hat{X})
\end{align}
If the solution to the PDE system  exists, then finding the solution is equivalent to minimizing the following functional:
\begin{align}
\label{eq:4}
\mathcal{J}(u) &= \int_{\Omega}
\end{align}
leading to the linear system
\begin{align}
\label{eq:linear-system}
\left< u_h , k(x_j , \cdot) \right>_{\mathcal{H}_k} &= \left< f , k(x_j , \cdot) \right>_{\mathcal{H}_k}
\end{align}
\begin{align}
\label{eq:5}
- \sum_{j=1}^{n} a_j \left( \left< \nabla_x a(x_i) , \nabla_x k(x_j,x_i)  \right> - a(x_i) \Delta_x k(x_j,x_i)\right) &= f(x_i)
\end{align}
** Preamble
#+begin_src julia :tangle src/kernel.jl :eval never

module Kernel
using StaticArrays
using KernelAbstractions
using LinearAlgebra
using ForwardDiff
#+end_src
** Linear Sytem
#+begin_src julia :tangle src/kernel.jl

@kernel function linear_matrix!(A ,@Const(X_L) , @Const(X) , k, ∇k , Δk , a , ∇a)
    # boilerplate
    Iᵢⱼ = @index(Global , Cartesian)
    @inbounds xᵢ= SVector{2}(view(X_L , : , Iᵢⱼ[1]))
    @inbounds xⱼ= SVector{2}(view(X , : , Iᵢⱼ[2]))
    # element computation
    @inbounds A[Iᵢⱼ] = ∇a(xᵢ)⋅∇k(xᵢ,xⱼ) -  a(xᵢ)Δk(xⱼ,xᵢ)
    end
#+end_src

#+RESULTS:
: linear_matrix! (generic function with 4 methods)

** Dirichlet boundary
The Dirichlet boundary confitions are dealt with as additional condition in the linear system
#+begin_src julia :tangle src/kernel.jl

@kernel function dirichlet_matrix!(A , @Const(X_D) , @Const(X) ,k)
    Iᵢⱼ =  @index(Global , Cartesian)
    @inbounds xᵢ= SVector{2}(view(X_D , : , Iᵢⱼ[1])) # Essentially X[:,1]
    @inbounds xⱼ= SVector{2}(view(X , : , Iᵢⱼ[2]))
    K = k(xᵢ , xⱼ)
    if isnan(K)
        @print(Iᵢⱼ , "\n")
        @print(xᵢ , "\n")
        @print(xⱼ , "\n")
        end
    @inbounds A[Iᵢⱼ] = K
end
#+end_src

#+RESULTS:
: julia-async:f35f44a7-4d8a-4825-9dd8-78915cf364bd
** Neumann Boundary

#+begin_src julia :tangle src/kernel.jl

@kernel function neumann_matrix!(A , @Const(X_N) , @Const(X) , @Const(N) , a , ∇k )
    Iᵢⱼ =  @index(Global , Cartesian)
    @inbounds xᵢ= SVector{2}(view(X_N , : , Iᵢⱼ[1])) # Essentially X[:,1]
    @inbounds xⱼ= SVector{2}(view(X , : , Iᵢⱼ[2]))
    @inbounds nᵢ= SVector{2}(view(N , : , Iᵢⱼ[1]))
    @inbounds A[Iᵢⱼ] = a(xᵢ) * (nᵢ ⋅ ∇k(xᵢ , xⱼ))
    end
#+end_src

#+RESULTS:
** right hand side
#+begin_src julia :tangle src/kernel.jl

@kernel function apply_function_colwise!(A ,@Const(X) , f::Function)
    # boilerplate
    Iᵢ = @index(Global , Cartesian)
    @inbounds xᵢ= SVector{2}(view(X , : , Iᵢ[1]))
    # element computation
    @inbounds A[Iᵢ] = f(xᵢ)
    end
#+end_src

#+RESULTS:

** Combined System

#+begin_src julia :tangle src/kernel.jl

@kernel function system_matrix!(A ,@Const(X), a , ∇a ,k, ∇k, Δk  , sdf , grad_sdf , sdf_beta)
    Iᵢⱼ =  @index(Global , Cartesian)
    @inbounds xᵢ= SVector{2}(view(X, : , Iᵢⱼ[1])) # Essentially X[:,1]
    @inbounds xⱼ= SVector{2}(view(X , : , Iᵢⱼ[2]))
    if sdf(xᵢ) < 1e-10
        if sdf_beta(xᵢ) < 0
            @inbounds nᵢ= grad_sdf(xᵢ)
            @inbounds A[Iᵢⱼ] = a(xᵢ) * (nᵢ ⋅ ∇k(xᵢ , xⱼ))
        else
           @inbounds A[Iᵢⱼ] =k(xᵢ , xⱼ)
        end
    else
        @inbounds A[Iᵢⱼ] = ∇a(xᵢ)⋅∇k(xᵢ,xⱼ) -  a(xᵢ)*Δk(xⱼ,xᵢ)
    end
    end
#+end_src
** Postable
#+begin_src julia :tangle src/kernel.jl :eval never

export linear_matrix!
export dirichlet_matrix!
export neumann_matrix!
export apply_function_colwise!
export system_matrix!
end
#+end_src
* Solver
#+begin_src julia :tangle src/pdesolver.jl :eval never
module PDESolvers

export PDESolver, PDESystem, solve
include("kernel.jl")

using .Kernel
using KernelAbstractions
using LinearAlgebra
#+end_src

#+RESULTS:

#+begin_src julia :tangle src/pdesolver.jl :eval never
struct PDESystem
    k :: Function
    ∇k :: Function
    Δk :: Function
    a :: Function
    ∇a::Function
    f::Function
    g_D::Function
    g_N::Function
    sdf::Function
    grad_sdf::Function
    sdf_beta::Function
end

struct PDESolver
    S::PDESystem
    X_L :: AbstractMatrix
    X_D :: AbstractMatrix
    X_N :: AbstractMatrix
    N :: AbstractMatrix
    α :: AbstractVector
end

function assemble_kernel_matrix(
    S,
    X_L :: AbstractMatrix ,
    X_D :: AbstractMatrix ,
    X_N :: AbstractMatrix ,
    N :: AbstractMatrix
)
    local X = [X_L X_D X_N]
    DOF = size(X,2)
    dev = get_backend(X)
    print("Backend" , dev)
    K = KernelAbstractions.zeros(dev , Float32,DOF ,DOF)
    print("Size of the system Matrix:" , size(K))
    K_linear = @view K[begin:size(X_L , 2) , :]
    K_dirichlet = @view K[size(X_L , 2)+1:end - size(X_N ,2), :]
    K_neumann = @view K[end-size(X_N ,2)+1:end, :]


    cpu_linear! = linear_matrix!( dev , 64 , size(K_linear))
    cpu_dirichlet! = dirichlet_matrix!( dev , 64 , size(K_dirichlet))
    cpu_neumann! = neumann_matrix!( dev , 64 , size(K_neumann))

    cpu_linear!(K_linear  , X_L , X , S.k , S.∇k , S.Δk , S.a , S.∇a)
    @info "Linear"
    cpu_dirichlet!(K_dirichlet  , X_D , X , S.k )
    @info "Dirichlet"
    cpu_neumann!(K_neumann  , X_N , X , N ,S.a, S.∇k)
    @info "Neumann"
    KernelAbstractions.synchronize(dev)
    return K
end
function solve(
    S,
    X_L :: AbstractMatrix ,
    X_D :: AbstractMatrix ,
    X_N :: AbstractMatrix ,
    N :: AbstractMatrix
    )
    K = assemble_kernel_matrix(S, X_L , X_D , X_N , N)
    b = get_boundary(S,X_L , X_D , X_N , N)
    @info "calulating pinv"
    α =  b'*pinv(K)
    @info "calculated pinv"
    return PDESolver(S, X_L , X_D , X_N , N , α' )
    #return b, K

    end
function (f::PDESolver)(X)
    local X_col = [f.X_L f.X_D f.X_N]
    dev = get_backend(X_col)
    print("Backend" , dev)
    K = KernelAbstractions.zeros(dev , Float32, size(X,2)  , size(X_col ,2))
    print("Size of the system Matrix:" , size(K))
    kernel_matrix! = dirichlet_matrix!( dev , 256 , size(K))
    kernel_matrix!(K, X , X_col , f.S.k )
return K * f.α
end

function solve(S, X_col)
    dev = get_backend(X_col)
    K = KernelAbstractions.zeros(dev , Float32 , size(X_col , 2) , size(X_col , 2) )
    sys_matrix! = system_matrix!( dev , 256 , size(K))
    sys_matrix!(K ,X_col , S.a , S.∇a , S.k , S.∇k , S.Δk , S.sdf , S.grad_sdf , S.sdf_beta  )
    end

function get_boundary(
    S,
    X_L::AbstractMatrix ,
    X_D::AbstractMatrix ,
    X_N::AbstractMatrix,
    N::AbstractMatrix
    )
    dev = get_backend(X_L)
    F = KernelAbstractions.zeros(dev , Float32 , size(X_L , 2))
    G_D = KernelAbstractions.zeros(dev , Float32 , size(X_D , 2))
    G_N = KernelAbstractions.zeros(dev , Float32 , size(X_N , 2))
    apply! = apply_function_colwise!(dev , 256)
    apply!(F , X_L , S.f , ndrange=size(F))
    y = [S.f.(eachcol(X_L)); S.g_D.(eachcol(X_D)); S.g_N.(eachcol(X_N) , eachcol(N))]
    end

#+end_src

#+begin_src julia :tangle src/pdesolver.jl
end
#+end_src

* Domains
#+begin_src julia :tangle src/domains.jl :eval never
module Domains
using StaticArrays
using LinearAlgebra
using ForwardDiff
using Enzyme
export SquareDomain , LDomain , sdf_square , ∇sdf_square , unit_box_normals , unit_box_path , sdf_L , ∇sdf_L , sdf_β , sdf_L_grad , sdf_square_grad
#+end_src

** Utility
#+begin_src julia :tangle src/domains.jl :eval never
function unit_box_normals(γ::Float64)
    p = SVector{2}(0,0)
    xnormal = SVector{2}(1,0)
    ynormal = SVector{2}(0,1)
    branch = γ % 4.
    if floor(branch) == 0.
        n = -ynormal
    elseif floor(branch) == 1.
        n = xnormal
    elseif floor(branch) == 2.
        n = ynormal
    elseif floor(branch) == 3.
        n = -xnormal
    else
        throw("γ=$γ not in range [0 , 4]")
    end

    return n
end
function unit_box_path(γ::Float64)
    p = SVector{2}(0,0)
    xnormal = SVector{2}(1,0)
    ynormal = SVector{2}(0,1)
    branch = γ % 4.
    if floor(branch) == 0.
        p = branch%1 * xnormal
    elseif floor(branch) == 1.
        p = xnormal +  branch%1 * ynormal
    elseif floor(branch) == 2.
        p = (1-branch%1)*xnormal + ynormal
    elseif floor(branch) == 3.
        p = (1-branch%1) * ynormal
    else
        throw("γ=$γ not in range [0 , 4]")
    end
    return p
end
#+end_src

#+begin_src julia :tangle src/domains.jl :eval never

function sdf_square(x::SVector , r::Float64 , center::SVector)
    return norm(x-center,Inf) .- r
end

function sdf_L(x::SVector{2})
    return max(sdf_square(x , 1. , SVector(0,0)) , - sdf_square(x, 1. , SVector(1.,1.)))
end

function ∇sdf_L(x::SVector{2})
    ForwardDiff.gradient(sdf_L , x)
    return
end

function sdf_β(x::SVector{2})
    return sdf_square(x , 0.2 , SVector(-1.,-1) )
end
function sdf_square_grad(x::SVector{2}, r::Float64, center::SVector{2})
    d = x - center
    if abs(d[1]) > abs(d[2])
        return SVector(sign(d[1]), 0.0)
    elseif abs(d[2]) > abs(d[1])
        return SVector(0.0, sign(d[2]))
    else
        # Subgradient: pick any valid direction; here we average the two
        return normalize(SVector(sign(d[1]), sign(d[2])))
    end
end

function sdf_L_grad(x::SVector{2})
    f1 = sdf_square(x, 1.0, SVector(0.0, 0.0))
    f2 = -sdf_square(x, 1.0, SVector(1.0, 1.0))

    if f1 > f2
        return sdf_square_grad(x, 1.0, SVector(0.0, 0.0))
    elseif f2 > f1
        return -sdf_square_grad(x, 1.0, SVector(1.0, 1.0))  # negative because of the minus
    else
        # Subgradient — average of both directions
        g1 = sdf_square_grad(x, 1.0, SVector(0.0, 0.0))
        g2 = -sdf_square_grad(x, 1.0, SVector(1.0, 1.0))
        return normalize(g1 + g2)
    end
end
#+end_src

** Postable
#+begin_src julia :tangle src/domains.jl :eval never
end
#+end_src
* Results
#+begin_src julia
using Revise
includet("src/pdesolver.jl")
includet("src/domains.jl")
#+end_src

#+RESULTS:

#+begin_src julia
using .PDESolvers
using .Domains
#+end_src

#+RESULTS:

#+begin_src julia
S = PDESystem(k_gauss , ∇k_gauss , Δk_gauss , a, ∇a , f, g_D ,g_N , sdf_L , sdf_L_grad , sdf_β )
#+end_src

#+RESULTS:
: PDESystem(Main.k_gauss, Main.∇k_gauss, Main.Δk_gauss, Main.a, Main.∇a, Main.f, Main.g_D, Main.g_N, Main.Domains.sdf_L, Main.Domains.sdf_L_grad, Main.Domains.sdf_β)





#+begin_src julia :results silent
using Random
using CUDA
dev = cu
rng = MersenneTwister(0)
r = 0:0.2:1.99
N = unit_box_normals.(r)
N = reduce(hcat , N) |> dev
X_N = unit_box_path.(r)
X_N = reduce(hcat , X_N)|> dev
X_D = unit_box_path.(2:0.1:4)
X_D = reduce(hcat , X_D) |> dev
X_L = rand(rng , Float64, 2,100) |> dev

#+end_src


#+name: fig:collocation-points
#+begin_src julia :results file graphics :file "images/collocation-points.png"
using LaTeXStrings
using Makie
using GLMakie
fig = Figure()
ax = Axis(fig[1,1] , title="Collocation Points")

scatter!(ax,X_L |> Array, label="Data Points")
scatter!(ax,X_D|> Array, label="Dirichlet Points")
scatter!(ax,X_N |> Array, label="Neumann Points")
arrows!(ax,X_N[1,:]|> Array , X_N[2,:] |> Array, N[1,:] |> Array, N[2,:] |> Array, lengthscale=0.1)
axislegend(ax , position=:lt)
save("images/collocation-points.png",fig )
#+end_src

#+RESULTS: fig:collocation-points
[[file:images/collocation-points.png]]

#+begin_src julia
using LinearAlgebra
solution = solve(S , X_L , X_D , X_N , N)
#+end_src

#+RESULTS:
: julia-async:a974108b-8bfd-4f92-8392-33b6433a07d9

#+name: fig:solution
#+begin_src julia :results file graphics :file "images/solution.png"
using GLMakie
X = range(-2 , 2 , 100)
Y = range(-2 , 2 , 100)
grid = [ [x,y] for x in X , y in Y]
grid = reduce(vcat , grid)
grid = reshape(grid, 2,:)
fig = Figure()
ax = Axis(fig[1,1])
sol = solution(grid)
sol = reshape(sol , size(X,1) , :)
hm = heatmap!(ax , X,Y, sol)
Colorbar(fig[:, end+1], hm)
save("images/solution.png",fig )
#+end_src

#+RESULTS: fig:solution
[[file:images/solution.png]]
